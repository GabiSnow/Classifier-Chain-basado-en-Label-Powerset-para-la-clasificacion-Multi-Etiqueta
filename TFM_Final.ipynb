{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabiSnow/ClassifierChainModification/blob/main/TFM_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJz-T_nQZKbs"
      },
      "source": [
        "**Intalaciones e Includes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-fpz2GLZCMY",
        "outputId": "f399bd9e-5efc-480e-e3bb-21407b82fd3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-multilearn==0.2.0 in /usr/local/lib/python3.10/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-multilearn==0.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIP8feoqkeZF",
        "outputId": "a37dcb80-b9e4-4274-ab13-3da9db53811b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: arff in /usr/local/lib/python3.10/dist-packages (0.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install arff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q3nW8cFZRHr"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "import statistics\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFQyNoBDZiyp"
      },
      "source": [
        "#**Declaracion de balanced K Means**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pcl7XA2arnj7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import networkx as nx\n",
        "from networkx.algorithms.community.kernighan_lin import kernighan_lin_bisection\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.preprocessing import normalize as normalize_features\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "\n",
        "class BalancedKMeansPartitioner():\n",
        "\n",
        "\tdef __init__(self,num_partitions=2,tol=0.003,max_iter=300):\n",
        "\t\tself.num_partitions=num_partitions\n",
        "\t\tself.tol=tol\n",
        "\t\tself.max_iter=max_iter\n",
        "\n",
        "\tdef partition(self,x_mat):\n",
        "\t\tnum_samples=x_mat.shape[0]\n",
        "\t\tassert(x_mat.shape[0]==num_samples)\n",
        "\t\tnum_dims=x_mat.shape[1]\n",
        "\t\t# base case\n",
        "\t\tif self.num_partitions >= num_samples:\n",
        "\t\t\tpadding = [[] for i in range(0,self.num_partitions-num_samples)]\n",
        "\t\t\tone_per_part=[[idx] for idx in range(0,num_samples)]\n",
        "\t\t\treturn one_per_part + padding\n",
        "\t\t# maximum size of each balanced cluster\n",
        "\t\tmax_size=np.ceil(num_samples/self.num_partitions)\n",
        "\t\t# initialize means\n",
        "\t\tpermute=np.random.permutation(num_samples)\n",
        "\t\tmeans=x_mat[permute[:self.num_partitions],:]\n",
        "\t\tassert(means.shape[0]==self.num_partitions)\n",
        "\t\t# initialize loop vars\n",
        "\t\tnew_means=np.zeros(means.shape)\n",
        "\t\tdiff=np.inf\n",
        "\t\tnum_iter=0\n",
        "\t\tsorted_clusters=[None]*self.num_partitions\n",
        "\t\t# loop to convergence / limit\n",
        "\t\twhile diff > self.tol and num_iter < self.max_iter:\n",
        "\t\t\t# calculate distances between means and labels\n",
        "\t\t\t# numpy voodoo\n",
        "\t\t\tdist= euclidean_distances(x_mat,means)\n",
        "\t\t\tassert(dist.shape==(num_samples,self.num_partitions))\n",
        "\t\t\tdist=np.sqrt(dist)\n",
        "\n",
        "\t\t\t# initialize sorted (w.r.t distance from mean)\n",
        "\t\t\tfor i in range(0,self.num_partitions):\n",
        "\t\t\t \tsorted_clusters[i]=[]\n",
        "\t\t\t# assign new labels keeping clusters balanced\n",
        "\t\t\tfor idx in range(0,num_samples):\n",
        "\t\t\t\tfinished=False\n",
        "\t\t\t\t# insert idx into its best cluster, possibly setting off a cascade to maintain\n",
        "\t\t\t\t# size of cluster < max_size\n",
        "\t\t\t\tins_idx=idx\n",
        "\t\t\t\twhile not finished:\n",
        "\t\t\t\t\tbest_cluster=np.argmin(dist[ins_idx,:])\n",
        "\t\t\t\t\tsorted_clusters[best_cluster]=self._insert_in_sorted_list(sorted_clusters[best_cluster],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t(ins_idx,dist[ins_idx,best_cluster]))\n",
        "\t\t\t\t\t# check if size is still ok\n",
        "\t\t\t\t\tif len(sorted_clusters[best_cluster])<=max_size:\n",
        "\t\t\t\t\t\t# done with the cascades (if any)\n",
        "\t\t\t\t\t\tfinished=True\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t# remove the worst/last label from this cluster\n",
        "\t\t\t\t\t\tins_idx=sorted_clusters[best_cluster].pop(-1)[0]\n",
        "\t\t\t\t\t\tdist[ins_idx,best_cluster]=np.inf\n",
        "\n",
        "\t\t\t# calculate new means\n",
        "\t\t\tfor i in range(0,self.num_partitions):\n",
        "\t\t\t\tidcs = [el[0] for el in sorted_clusters[i]]\n",
        "\t\t\t\t# print(x_mat[idcs,:].shape)\n",
        "\t\t\t\tnew_means[i,:]=np.mean(x_mat[idcs,:],axis=0)\n",
        "\t\t\t# update\n",
        "\t\t\tdiff=np.linalg.norm(means-new_means,ord=2)\n",
        "\t\t\tmeans=new_means\n",
        "\t\t\tnum_iter=num_iter+1\n",
        "\t\tpartitions=[None]*self.num_partitions\n",
        "\t\tfor i in range(0,len(sorted_clusters)):\n",
        "\t\t\tpartitions[i]=[j for (j,dist) in sorted_clusters[i]]\n",
        "\t\treturn partitions\n",
        "\n",
        "\tdef _insert_in_sorted_list(self,sorted_list,element):\n",
        "\t\t# insert element into a sorted list of (sample_id,sorting_key) pairs\n",
        "\t\t_,sorting_key=element\n",
        "\t\t# dummy element for easier indexing\n",
        "\t\tsorted_list=[(-50,-np.inf)]+sorted_list\n",
        "\t\t# linear search for correct position\n",
        "\t\tfor i in range(0,len(sorted_list)):\n",
        "\t\t\t# reached the last position\n",
        "\t\t\tif i==len(sorted_list)-1:\n",
        "\t\t\t\tsorted_list.append(element)\n",
        "\t\t\t\tbreak\n",
        "\t\t\tif sorted_list[i][1]<=sorting_key and sorted_list[i+1][1]>sorting_key:\n",
        "\t\t\t\tsorted_list=sorted_list[:i+1]+[element]+sorted_list[i+1:]\n",
        "\t\t\t\tinserted=True\n",
        "\t\t\t\tbreak\n",
        "\t\tsorted_list=sorted_list[1:]\n",
        "\t\treturn sorted_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH6Pb4TGZltN"
      },
      "source": [
        "#**ClassifierChain basado en powersets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4zz2cqkZrVL"
      },
      "outputs": [],
      "source": [
        "class ClassChain_Powerset ():\n",
        "  lista_clasificadores = [] # nuestro modelo\n",
        "  etiquetas = [] # debemos guardar a que cluster pertenece cada etiqueta\n",
        "  nClusters = 2  # Numero de clusters // hay que calcularlo, lo que necesito es un tamaño de clusters de 5\n",
        "\n",
        "  def __init__(self,X_trainF,Y_trainF):\n",
        "    self.aplicar_kmeans_etiquetas(Y_trainF)\n",
        "    self.entrenar_modelo(X_trainF,Y_trainF)\n",
        "\n",
        "  def entrenar_modelo(self,X_trainF,Y_trainF): # tengo que ajustar los parametros\n",
        "\n",
        "    self.lista_clasificadores = []\n",
        "    Y_train_list = self.separar_etiquetas(Y_trainF) # separo la matriz de etiquetas en una lista de matrices\n",
        "    X_train_aux = X_trainF.copy()\n",
        "\n",
        "    for i in range(self.nClusters):\n",
        "      #m = ClassifierChain(LogisticRegression(max_iter=150)) # aqui tengo que declarar un clasificador multiclase, cuya salida es el powerset\n",
        "      m = LabelPowerset(RandomForestClassifier(n_estimators=25))\n",
        "      #print(Y_train_list[i])\n",
        "      m.fit(X_train_aux , Y_train_list[i])\n",
        "      self.lista_clasificadores += [m]\n",
        "\n",
        "      # concatenación del los resultados y el dataset\n",
        "      X_train_aux = np.concatenate((Y_train_list[i], X_train_aux),axis=1)\n",
        "\n",
        "# hecho\n",
        "  def predict(self,X_test):\n",
        "    # cada clasificador predice un cluster distinto de etiquetas\n",
        "    #lista_predicciones = []\n",
        "    X_test_aux = X_test.copy()\n",
        "\n",
        "    Y = np.zeros([X_test_aux.shape[0],self.etiquetas.shape[0]]) # creo la Y de salida\n",
        "\n",
        "    # Sacamos las etiquetas predichas por cada clasificador\n",
        "    for i in range(self.nClusters):\n",
        "      # toarrayt() porque devolvia una matriz de dispersion\n",
        "      labelsPred = self.lista_clasificadores[i].predict(X_test_aux).toarray() # etiquetas predichas por cada clasificado\n",
        "\n",
        "      # le hago la transpuesta para poder aplicar lo de buscar etiquetas\n",
        "      Y = np.transpose(Y) #esto deberia hacerlo fuera\n",
        "      Y[self.etiquetas == i] = np.transpose(labelsPred)\n",
        "      Y = np.transpose(Y) # la devuelvo al estado normal\n",
        "\n",
        "      # concatenación del los resultados y el dataset\n",
        "      X_test_aux = np.concatenate((labelsPred, X_test_aux),axis=1)\n",
        "\n",
        "    return Y\n",
        "\n",
        "  # Hecho\n",
        "  # Este apartado nos divide las etiquetas en n clusters\n",
        "  def aplicar_kmeans_etiquetas(self,labels):\n",
        "    labels_tp = np.transpose(labels)\n",
        "    num_lb = labels_tp.shape[0]\n",
        "    self.nClusters = int(num_lb/5)\n",
        "\n",
        "    if(num_lb % 5 != 0):\n",
        "      self.nClusters +=1\n",
        "\n",
        "    k_means = BalancedKMeansPartitioner(num_partitions=self.nClusters)\n",
        "    particion = k_means.partition(labels_tp)\n",
        "    #k_means = balanced_kmeans(X = np.transpose(labels), n_clusters=self.nClusters)\n",
        "    print(particion)\n",
        "\n",
        "    self.etiquetas = self.cambioCadena(particion,num_lb)\n",
        "\n",
        "# Hecho\n",
        "# divide nuestra matriz de etiquetas en una lista de matrices de etiquetas (una por cada cluster)\n",
        "  def separar_etiquetas(self,labels):\n",
        "    lista = []\n",
        "    for i in range(self.nClusters):\n",
        "      aux = np.transpose(labels)\n",
        "      lista += [np.transpose(aux[self.etiquetas==i])]\n",
        "    return lista\n",
        "\n",
        "  def cambioCadena(self,etiquetas_cluster,num_lb):\n",
        "    # Inicializa la lista resultado con 0.\n",
        "    resultado = np.zeros(num_lb)\n",
        "\n",
        "    # Asigna el número de cluster a cada elemento en la base de datos original.\n",
        "    for cluster_num, elementos_cluster in enumerate(etiquetas_cluster):\n",
        "        for elemento_pos in elementos_cluster:\n",
        "            resultado[elemento_pos] = cluster_num\n",
        "    return resultado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS8OR7mptSSh"
      },
      "source": [
        "#**Homer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yhnt1_PNoW3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "\n",
        "class LabelTree:\n",
        "\n",
        "\tdef __init__(self, partitioner, leaf_classifier,\n",
        "\t\t\t\t\tinternal_classifier, stopping_condition):\n",
        "\t\t# tree params\n",
        "\t\tself.partitioner=partitioner\n",
        "\t\tself.stopping_condition=stopping_condition\n",
        "\t\t# classification params\n",
        "\t\tself.leaf_classifier=leaf_classifier\n",
        "\t\tself.internal_classifier=internal_classifier\n",
        "\t\t# initialize tree params\n",
        "\t\tself.root=None\n",
        "\t\tself.nodes=[]\n",
        "\n",
        "\tdef _get_new_node(self,parent,node_labels,train_idcs,depth):\n",
        "\t\tnew_node=LabelTreeNode(self,parent,node_labels,train_idcs,depth)\n",
        "\t\tnew_node.idx=len(self.nodes)\n",
        "\t\tself.nodes.append(new_node)\n",
        "\t\treturn new_node\n",
        "\n",
        "\tdef fit(self,x_mat,y_mat,repre):\n",
        "\t\tassert(x_mat.shape[0]==y_mat.shape[0])\n",
        "\t\tassert(repre.shape[0]==y_mat.shape[1])\n",
        "\t\tself.num_features=x_mat.shape[1]\n",
        "\t\tself.num_labels=y_mat.shape[1]\n",
        "\t\tself.num_trn_points=x_mat.shape[0]\n",
        "\t\tself._fit_tree(x_mat,y_mat,repre)\n",
        "\t\tself._fit_classifiers(x_mat,y_mat)\n",
        "\n",
        "\tdef _fit_tree(self,x_mat,y_mat,repre):\n",
        "\t\tall_idcs=list(range(0,self.num_trn_points))\n",
        "\t\tall_labels=list(range(0,self.num_labels))\n",
        "\t\tself.root=self._get_new_node(None,all_labels,all_idcs,0)\n",
        "\t\tself.root._split(x_mat,y_mat,repre)\n",
        "\n",
        "\tdef _fit_classifiers(self,x_mat,y_mat):\n",
        "\t\tprint(\"Fitting \",len(self.nodes),\" classifiers...\")\n",
        "\t\tfor i,node in enumerate(self.nodes):\n",
        "\t\t\tnode._fit(x_mat,y_mat)\n",
        "\t\tprint(\"Done fitting\")\n",
        "\n",
        "\tdef predict_proba(self,x_tst,method=\"beam_search\",num_paths=10,recurse_threshold=0.5):\n",
        "\t\tassert(x_tst.shape[1]==self.num_features)\n",
        "\t\tassert(method in [\"beam_search\",\"recursive\"])\n",
        "\t\tif method==\"recursive\":\n",
        "\t\t\tprobs=np.zeros((x_tst.shape[0],self.num_labels))\n",
        "\t\t\ttst_idcs=list(range(0,x_tst.shape[0]))\n",
        "\t\t\tself.root._predict_proba(x_tst,probs,tst_idcs,recurse_threshold)\n",
        "\t\t\treturn probs\n",
        "\t\telif method==\"beam_search\":\n",
        "\t\t\treturn self._predict_proba_beam_search(x_tst,num_paths)\n",
        "\n",
        "\tdef predict(self,x_tst,threshold=0.5,method=\"beam_search\",num_paths=10,\n",
        "\t\t\t\trecurse_threshold=0.5,return_probs=False):\n",
        "\t\tprobs=self.predict_proba(x_tst,method,num_paths,recurse_threshold)\n",
        "\t\ty_pred=(probs>threshold)*1\n",
        "\t\tif return_probs:\n",
        "\t\t\treturn y_pred,probs\n",
        "\t\telse:\n",
        "\t\t\treturn y_pred\n",
        "\n",
        "\tdef _predict_proba_beam_search(self,x_tst,num_paths):\n",
        "\t\t# vectorized beam search! way faster than calling per sample\n",
        "\t\t# but bug : in case of ties\n",
        "\t\tnum_nodes=len(self.nodes)\n",
        "\t\tnum_samples=x_tst.shape[0]\n",
        "\t\t# for each sample store the boundary as an array of path probs (to a node)\n",
        "\t\tboundary=np.zeros((num_samples,num_nodes))\n",
        "\t\tsearch_done=np.zeros(num_samples).astype(bool)\n",
        "\t\t# initialize loop variables\n",
        "\t\tboundary[:,self.root.idx]=1.0\n",
        "\t\tnum_iter=0\n",
        "\t\twhile not np.all(search_done):\n",
        "\t\t\t# get nodes which have at least one sample at it, and set search_done to true\n",
        "\t\t\tactive_nodes_list=np.nonzero(np.sum(boundary,axis=0)>0)[0]\n",
        "\t\t\t# expand internal nodes in active_nodes_list\n",
        "\t\t\tsearch_done[:]=True\n",
        "\t\t\tfor node_idx in active_nodes_list:\n",
        "\t\t\t\tnode=self.nodes[node_idx]\n",
        "\t\t\t\t# active nodes which are leaves are skipped\n",
        "\t\t\t\t# ~ thus for points at leaves, search_done=True\n",
        "\t\t\t\tif node.node_type==\"leaf\":\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\t# otherwise expand this node and mark search_done=False for its points\n",
        "\t\t\t\tchildren_global_idcs=[self.nodes.index(ch) for ch in node.children]\n",
        "\t\t\t\tactive_points=np.nonzero(boundary[:,node_idx])[0]\n",
        "\t\t\t\tsearch_done[active_points]=False\n",
        "\t\t\t\t# route points to children by assigning values to\n",
        "\t\t\t\trouting_probs=node.classifier.predict_proba(\n",
        "\t\t\t\t\t\t\t\t\tx_tst[active_points,:].reshape((-1,self.num_features)))\n",
        "\t\t\t\tassert(routing_probs.shape==(len(active_points),len(children_global_idcs)))\n",
        "\t\t\t\tpath_probs= (routing_probs.T * boundary[active_points,node_idx] ).T\n",
        "\t\t\t\tfor ch_par_idx,ch_glo_idx in enumerate(children_global_idcs):\n",
        "\t\t\t\t\tboundary[active_points,ch_glo_idx]=path_probs[:,ch_par_idx]\n",
        "\t\t\t\t# erase parent from boundary\n",
        "\t\t\t\tboundary[active_points,node_idx]=0\n",
        "\t\t\t# filter out all except best num_paths nodes for each sample\n",
        "\t\t\tbest_nodes_per_sample=np.argsort(boundary,axis=1)[:,-num_paths:]\n",
        "\t\t\tnew_boundary=boundary.copy()*0\n",
        "\t\t\tfor s_idx in range(0,num_samples):\n",
        "\t\t\t\tfor best_idx in range(0,num_paths):\n",
        "\t\t\t\t\tbest_node_idx=best_nodes_per_sample[s_idx,-(best_idx+1)]\n",
        "\t\t\t\t\t# boundary is < num_paths\n",
        "\t\t\t\t\tif boundary[s_idx,best_node_idx]==0:\n",
        "\t\t\t\t\t\tbreak\n",
        "\t\t\t\t\tnew_boundary[s_idx,best_node_idx]=boundary[s_idx,best_node_idx]\n",
        "\t\t\tboundary=new_boundary\n",
        "\t\t\tnum_iter+=1\n",
        "\n",
        "\t\t# the active_nodes_list should be just leaves now, so lets fill the final probability arr\n",
        "\t\t# by multiplying path probs (boundary) with leaf probs\n",
        "\t\tactive_nodes_list=np.nonzero(np.sum(boundary,axis=0)>0)[0]\n",
        "\t\tprobs=np.zeros((num_samples,self.num_labels))\n",
        "\t\tfor node_idx in active_nodes_list:\n",
        "\t\t\tnode=self.nodes[node_idx]\n",
        "\t\t\tassert(node.node_type==\"leaf\")\n",
        "\t\t\tactive_points=np.nonzero(boundary[:,node_idx])[0]\n",
        "\t\t\tprobs_leaf=node.classifier.predict_proba(x_tst[active_points,:].reshape((-1,self.num_features)))\n",
        "\t\t\ttotal_probs=(probs_leaf.T * boundary[active_points,node_idx] )\n",
        "\t\t\t# assign probs\n",
        "\t\t\tfor i,lab in enumerate(node.labels):\n",
        "\t\t\t\tprobs[active_points,lab]=total_probs[:,i]\n",
        "\t\treturn probs\n",
        "\n",
        "\tdef walk_tree(self,walker):\n",
        "\t\tself.root._walk(walker)\n",
        "\n",
        "\n",
        "class LabelTreeNode:\n",
        "\n",
        "\tdef __init__(self,tree,parent,node_labels,train_idcs,depth):\n",
        "\t\t# labels values refer to original labelset (0,y_mat.shape[1]-1)\n",
        "\t\tself.labels=list(node_labels)\n",
        "\t\t# x_idcs refer to the training set (x_mat,y_mat)\n",
        "\t\tself.train_idcs=list(train_idcs)\n",
        "\t\tself.tree=tree\n",
        "\t\tself.depth=depth\n",
        "\t\tself.parent=parent\n",
        "\t\tself.node_type=None\n",
        "\t\tself.children=[]\n",
        "\t\tself.classifier=None\n",
        "\n",
        "\tdef _check_partitions(self,lparts,dparts):\n",
        "\t\tunion_labels=set()\n",
        "\t\tunion_data=set()\n",
        "\t\tfor i in range(0,len(dparts)):\n",
        "\t\t\tunion_labels=union_labels|set(lparts[i])\n",
        "\t\t\tunion_data=union_data|set(dparts[i])\n",
        "\t\tassert(union_labels==set(self.labels))\n",
        "\t\tif self.parent is not None:\n",
        "\t\t\tassert(union_data==set(self.train_idcs))\n",
        "\n",
        "\tdef _split(self,x_mat,y_mat,repre):\n",
        "\t\t# check if we have reached a leaf\n",
        "\t\tif self.tree.stopping_condition.check(self,x_mat,y_mat,repre):\n",
        "\t\t\tself.node_type=\"leaf\"\n",
        "\t\t\treturn\n",
        "\t\tself.node_type=\"internal\"\n",
        "\t\t# partition labels and filter empty partiyions\n",
        "\t\tlparts_temp=self.tree.partitioner.partition(repre[self.labels,:])\n",
        "\t\tlparts=[]\n",
        "\t\tfor lpart in lparts_temp:\n",
        "\t\t\tif lpart==[]:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\t# translate to original indices\n",
        "\t\t\tlp=[self.labels[lab] for lab in lpart]\n",
        "\t\t\tlparts.append(lp)\n",
        "\t\t# partition data\n",
        "\t\tdparts=self._partition_data_by_labels(lparts,y_mat)\n",
        "\t\tself._check_partitions(lparts,dparts)\n",
        "\t\tif len(dparts)<=1:\n",
        "\t\t\tself.node_type=\"leaf\"\n",
        "\t\t\treturn\n",
        "\t\t# for each partition instantiate a new node\n",
        "\t\tfor i in range(0,len(lparts)):\n",
        "\t\t\tchild=self.tree._get_new_node(self,lparts[i],dparts[i],self.depth+1)\n",
        "\t\t\tself.children.append(child)\n",
        "\t\t\tchild._split(x_mat,y_mat,repre)\n",
        "\n",
        "\tdef _partition_data_by_labels(self,label_partitions,y_mat):\n",
        "\t\t# check that all labels are valid indices\n",
        "\t\tfor lpart in label_partitions:\n",
        "\t\t\tfor label in lpart:\n",
        "\t\t\t\tassert(label<y_mat.shape[1])\n",
        "\t\t# partition data\n",
        "\t\tdata_partitions=[]\n",
        "\t\tfor lpart in label_partitions:\n",
        "\t\t\tif lpart==[]:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\t# get points where at least one of the labels is active / =1\n",
        "\t\t\tactive_bool=np.sum(y_mat[:,lpart],axis=1) > 0\n",
        "\t\t\tdata_idcs=np.nonzero(active_bool)[0]\n",
        "\t\t\tdata_partitions.append(data_idcs.tolist())\n",
        "\t\treturn data_partitions\n",
        "\n",
        "\tdef _fit(self,x_mat,y_mat):\n",
        "\t\tif self.node_type==\"leaf\":\n",
        "\t\t\tself.classifier=copy.deepcopy(self.tree.leaf_classifier)\n",
        "\t\telse:\n",
        "\t\t\tself.classifier=copy.deepcopy(self.tree.internal_classifier)\n",
        "\t\ty_mat_new=None\n",
        "\t\tif self.node_type==\"internal\":\n",
        "\t\t\t# for internal nodes, generate pseudo labels for children\n",
        "\t\t\ty_mat_new=np.zeros((y_mat.shape[0],len(self.children)))\n",
        "\t\t\tfor i,child in enumerate(self.children):\n",
        "\t\t\t\ty_mat_new[child.train_idcs,i]=1\n",
        "\t\t\t# restrict to active points only\n",
        "\t\t\ty_mat_new=y_mat_new[self.train_idcs,:]\n",
        "\t\telif self.node_type==\"leaf\":\n",
        "\t\t\t# restrict to active points only with active labels\n",
        "\t\t\ty_mat_new=y_mat[self.train_idcs,:][:,self.labels]\n",
        "\t\t# restrict x_mat to active points only\n",
        "\t\tx_mat_new=x_mat[self.train_idcs,:]\n",
        "\t\tself.classifier.fit(x_mat_new,y_mat_new)\n",
        "\n",
        "\tdef _predict_proba(self,x_tst_global,probs_global,tst_idcs,recurse_threshold):\n",
        "\t\tif self.node_type==\"internal\":\n",
        "\t\t\trouting_probs=self.classifier.predict_proba(x_tst_global[tst_idcs,:])\n",
        "\t\t\trouting_labels=(routing_probs>recurse_threshold)*1\n",
        "\t\t\tfor ch_idx,child in enumerate(self.children):\n",
        "\t\t\t\tchild_tst_idcs=np.nonzero(routing_labels[:,ch_idx]>0)[0]\n",
        "\t\t\t\t# translate to original array\n",
        "\t\t\t\tchild_tst_idcs=[tst_idcs[idx] for idx in child_tst_idcs]\n",
        "\t\t\t\tif len(child_tst_idcs)>0:\n",
        "\t\t\t\t\tchild._predict_proba(x_tst_global,probs_global,child_tst_idcs,recurse_threshold)\n",
        "\t\tif self.node_type==\"leaf\":\n",
        "\t\t\tprobs_leaf=self.classifier.predict_proba(x_tst_global[tst_idcs,:])\n",
        "\t\t\tfor i,lab in enumerate(self.labels):\n",
        "\t\t\t\tprobs_global[tst_idcs,lab]=probs_leaf[:,i]\n",
        "\n",
        "\tdef _walk(self,walker):\n",
        "\t\twalker.process_node(self)\n",
        "\t\tfor child in self.children:\n",
        "\t\t\tchild._walk(walker)\n",
        "\n",
        "\n",
        "class LeafSizeStoppingCondition:\n",
        "\n",
        "\tdef __init__(self,min_leaf_size):\n",
        "\t\tassert(min_leaf_size>0)\n",
        "\t\tself.min_leaf_size=min_leaf_size\n",
        "\n",
        "\tdef check(self,lnode,x_mat,y_mat,repre):\n",
        "\t\treturn len(lnode.labels) <= self.min_leaf_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_GHoZzBxl2D"
      },
      "outputs": [],
      "source": [
        "def generate_parabel_label_representations(x_mat,y_mat):\n",
        "\tassert(y_mat.shape[0]==x_mat.shape[0])\n",
        "\tx_dim=x_mat.shape[1]\n",
        "\tnum_labels=y_mat.shape[1]\n",
        "\trepre=np.zeros((num_labels,x_dim))\n",
        "\t# for each label calculate the mean of its support\n",
        "\tfor l in range(0,num_labels):\n",
        "\t\tsupp_idcs=np.nonzero(y_mat[:,l]>0)[0]\n",
        "\t\tif len(supp_idcs)==0:\n",
        "\t\t\tprint(\"Warning! Label \",l,\"has no positive examples. Setting representation to 0.\")\n",
        "\t\t\tcontinue\n",
        "\t\tlabel_support=x_mat[supp_idcs,:]\n",
        "\t\tmean_support_vec=np.mean(label_support,axis=0)\n",
        "\t\trepre[l,:]=mean_support_vec\n",
        "\t# normalize features after\n",
        "\trepre=normalize_features(repre)\n",
        "\treturn repre\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlGw_50rQcHl"
      },
      "source": [
        "**Declaracion de Classifier Chain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTQsc_8h8VO8"
      },
      "outputs": [],
      "source": [
        "class ClassifierChainClass:\n",
        "\n",
        "\tdef __init__(self,max_itera=150):\n",
        "\t\t# base classifier should be like an sklearn classifier\n",
        "\t\t# importantly support fit(x_mat,)\n",
        "\t\tself.classifier=ClassifierChain(LogisticRegression(max_iter=max_itera))\n",
        "\t\tself.num_labels=None\n",
        "\t\tself.classifier_list=None\n",
        "\n",
        "\tdef fit(self,x_mat,y_mat):\n",
        "\t\tself.classifier.fit(x_mat,y_mat)\n",
        "\n",
        "\tdef predict_proba(self,x_tst):\n",
        "\t\treturn self.classifier.predict_proba(x_tst).toarray()\n",
        "\n",
        "\tdef predict(self,x_tst):\n",
        "\t\t\treturn self.classifier.predict(x_tst).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN0QG08YPOJ8"
      },
      "source": [
        "#**Pruebas de rendimiento**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8mDouXlmOsO"
      },
      "outputs": [],
      "source": [
        "from skmultilearn.problem_transform import ClassifierChain, BinaryRelevance\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, jaccard_score\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEnhAiTE_D_g"
      },
      "outputs": [],
      "source": [
        "def evaluar_dataset(X_train,Y_train,X_test,Y_test,max_itera=150,leaf_cond=5):\n",
        "\n",
        "  cc_train_time = 0\n",
        "  cc_test_time = 0\n",
        "  cc_f1 = 0\n",
        "  cc_acc = 0\n",
        "\n",
        "  pw_train_time = 0\n",
        "  pw_test_time = 0\n",
        "  pw_f1 = 0\n",
        "  pw_acc = 0\n",
        "\n",
        "  lb_train_time = 0\n",
        "  lb_test_time = 0\n",
        "  lb_f1 = 0\n",
        "  lb_acc = 0\n",
        "\n",
        "  num = 3\n",
        "  for x in range(num):\n",
        "    # Declaro un classifier chain normal\n",
        "    cc = ClassifierChain(LogisticRegression(max_iter=max_itera))\n",
        "    #cc = ClassifierChain(RandomForestClassifier(n_estimators=25))\n",
        "    ini = time.time()\n",
        "    cc.fit(X_train, Y_train)\n",
        "    fin = time.time()\n",
        "    cc_train_time += fin - ini\n",
        "\n",
        "    ini = time.time()\n",
        "    cc_Y_pred = cc.predict(X_test).toarray()\n",
        "    fin = time.time()\n",
        "    cc_test_time += fin - ini\n",
        "\n",
        "    cc_f1 += f1_score(Y_test,cc_Y_pred, average='micro')\n",
        "    cc_acc += jaccard_score(Y_test,cc_Y_pred, average='micro')\n",
        "\n",
        "    # Declaro un classifier chain basado en powersets\n",
        "    ini = time.time()\n",
        "    pw = ClassChain_Powerset(X_train,Y_train)\n",
        "    fin = time.time()\n",
        "    pw_train_time += fin - ini\n",
        "\n",
        "    ini = time.time()\n",
        "    pw_Y_pred = pw.predict(X_test)\n",
        "    fin = time.time()\n",
        "    pw_test_time += fin - ini\n",
        "\n",
        "    pw_f1 += f1_score(Y_test,pw_Y_pred, average='micro')\n",
        "    pw_acc += jaccard_score(Y_test,pw_Y_pred, average='micro')\n",
        "\n",
        "    # Declaracion de homer\n",
        "    repre = generate_parabel_label_representations(X_train,Y_train)\n",
        "\n",
        "    lb = LabelTree(BalancedKMeansPartitioner(), ClassifierChainClass(max_itera),ClassifierChainClass(max_itera), LeafSizeStoppingCondition(leaf_cond))\n",
        "\n",
        "    ini = time.time()\n",
        "    lb.fit(X_train,Y_train,repre)\n",
        "    fin = time.time()\n",
        "    lb_train_time += fin - ini\n",
        "\n",
        "    ini = time.time()\n",
        "    lb_Y_pred = lb.predict(X_test,method=\"recursive\",num_paths=10,recurse_threshold=0.5)\n",
        "    fin = time.time()\n",
        "    lb_test_time += fin - ini\n",
        "\n",
        "    lb_f1 += f1_score(Y_test,lb_Y_pred, average='micro')\n",
        "    lb_acc += jaccard_score(Y_test,lb_Y_pred, average='micro')\n",
        "    # Main\n",
        "\n",
        "\n",
        "  print(\"-----------------------\")\n",
        "  print(\"Classifier Chain\")\n",
        "  print(f\"Tiempo entrenamiento: {cc_train_time/num}\")\n",
        "  print(f\"Tiempo prediccion: {cc_test_time/num}\")\n",
        "  print(f\"F1 Score: {cc_f1/num}\")\n",
        "  print(f\"Accuracy: {cc_acc/num}\")\n",
        "\n",
        "  print(\"\\n-----------------------\")\n",
        "  print(\"Powerset Classifier Chain\")\n",
        "  print(f\"Tiempo entrenamiento: {pw_train_time/num}\")\n",
        "  print(f\"Tiempo prediccion: {pw_test_time/num}\")\n",
        "  print(f\"F1 Score: {pw_f1/num}\")\n",
        "  print(f\"Accuracy: {pw_acc/num}\")\n",
        "\n",
        "\n",
        "  print(\"\\n-----------------------\")\n",
        "  print(\"Homer Class Chain\")\n",
        "  print(f\"Tiempo entrenamiento: {lb_train_time/num}\")\n",
        "  print(f\"Tiempo prediccion: {lb_test_time/num}\")\n",
        "  print(f\"F1 Score: {lb_f1/num}\")\n",
        "  print(f\"Accuracy: {lb_acc/num}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvFdDjyY-_hB"
      },
      "source": [
        "#**Carga de datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38wuCt0cQsCT"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skmultilearn.dataset import load_dataset\n",
        "from sklearn.preprocessing import StandardScaler #Para escalar los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_HJiSEgb5oc",
        "outputId": "eb2109eb-abc7-4590-f029-7c652e524b78"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Corel5k',\n",
              " 'bibtex',\n",
              " 'birds',\n",
              " 'delicious',\n",
              " 'emotions',\n",
              " 'enron',\n",
              " 'genbase',\n",
              " 'mediamill',\n",
              " 'medical',\n",
              " 'rcv1subset1',\n",
              " 'rcv1subset2',\n",
              " 'rcv1subset3',\n",
              " 'rcv1subset4',\n",
              " 'rcv1subset5',\n",
              " 'scene',\n",
              " 'tmc2007_500',\n",
              " 'yeast'}"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "from skmultilearn.dataset import available_data_sets\n",
        "set([x[0] for x in available_data_sets().keys()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-2llWv6Q7BM"
      },
      "source": [
        "#**Scene**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdgPWz9lGoDR",
        "outputId": "d7c50658-491b-488f-f1c8-ea7d754fe96b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scene:train - exists, not redownloading\n",
            "scene:test - exists, not redownloading\n",
            "(1211, 294)\n",
            "(1211, 6)\n"
          ]
        }
      ],
      "source": [
        "# Scene Dataset\n",
        "X_train, Y_train, feature_names, label_names = load_dataset('scene', 'train')\n",
        "X_test, Y_test, feature_names, label_names = load_dataset('scene', 'test')\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "\n",
        "X_train = X_train.toarray()\n",
        "Y_train = Y_train.toarray()\n",
        "X_test = X_test.toarray()\n",
        "Y_test = Y_test.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0knhJAynuzz",
        "outputId": "425622b6-4f76-4320-ae51-da86c11a2095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 0, 4], [1, 3, 5]]\n",
            "Fitting  3  classifiers...\n",
            "Done fitting\n",
            "[[1, 3, 2], [0, 5, 4]]\n",
            "Fitting  3  classifiers...\n",
            "Done fitting\n",
            "[[1, 3, 2], [0, 5, 4]]\n",
            "Fitting  3  classifiers...\n",
            "Done fitting\n",
            "[[1, 3, 5], [2, 0, 4]]\n",
            "Fitting  3  classifiers...\n",
            "Done fitting\n",
            "[[1, 3, 5], [2, 0, 4]]\n",
            "Fitting  3  classifiers...\n",
            "Done fitting\n",
            "-----------------------\n",
            "Classifier Chain\n",
            "Tiempo entrenamiento: 0.6644410610198974\n",
            "Tiempo prediccion: 0.06157040596008301\n",
            "F1 Score: 0.7101159536185526\n",
            "Accuracy: 0.5505269683818971\n",
            "\n",
            "-----------------------\n",
            "Powerset Classifier Chain\n",
            "Tiempo entrenamiento: 1.0079029560089112\n",
            "Tiempo prediccion: 0.1577674388885498\n",
            "F1 Score: 0.7132513792975697\n",
            "Accuracy: 0.5544744776429196\n",
            "\n",
            "-----------------------\n",
            "Homer Class Chain\n",
            "Tiempo entrenamiento: 0.6488527774810791\n",
            "Tiempo prediccion: 0.11079916954040528\n",
            "F1 Score: 0.680870503688609\n",
            "Accuracy: 0.5161583861378245\n"
          ]
        }
      ],
      "source": [
        "evaluar_dataset(X_train,Y_train,X_test,Y_test,150,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JuK3gz-RAWu"
      },
      "source": [
        "#**Emotions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zc5nNKuTVae",
        "outputId": "afdf7577-8c1b-4d77-bcd5-2e175eb8533f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emotions:train - does not exists downloading\n",
            "Downloaded emotions-train\n",
            "emotions:test - does not exists downloading\n",
            "Downloaded emotions-test\n",
            "(391, 72)\n",
            "(391, 6)\n"
          ]
        }
      ],
      "source": [
        "# Emotions Dataset\n",
        "X_train, Y_train, feature_names, label_names = load_dataset('emotions', 'train')\n",
        "X_test, Y_test, feature_names, label_names = load_dataset('emotions', 'test')\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "#print(X_train.toarray())\n",
        "#print(Y_train.toarray())\n",
        "\n",
        "X_train = X_train.toarray()\n",
        "Y_train = Y_train.toarray()\n",
        "X_test = X_test.toarray()\n",
        "Y_test = Y_test.toarray()\n",
        "\n",
        "X_train = StandardScaler().fit_transform(X_train)\n",
        "X_test = StandardScaler().fit_transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlzFxAPs2IoN",
        "outputId": "1d304256-b07b-41ef-af92-fea61bbcbfdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de clases únicas: 2\n",
            "Clases únicas: [0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh56du3wn_0Z",
        "outputId": "304893b7-6ca4-401f-bb33-2435fdd404a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 5, 1], [3, 4, 2]]\n",
            "Fitting  3  classifiers...\n",
            "Done fitting\n",
            "[[4, 3, 0], [1, 2, 5]]\n",
            "Fitting  3  classifiers...\n",
            "Done fitting\n",
            "[[0, 5, 1], [3, 4, 2]]\n",
            "Fitting  3  classifiers...\n",
            "Done fitting\n",
            "[[0, 5, 4], [2, 3, 1]]\n",
            "Fitting  3  classifiers...\n",
            "Done fitting\n",
            "[[1, 2, 5], [4, 3, 0]]\n",
            "Fitting  3  classifiers...\n",
            "Done fitting\n",
            "-----------------------\n",
            "Classifier Chain\n",
            "Tiempo entrenamiento: 0.1836625099182129\n",
            "Tiempo prediccion: 0.008986711502075195\n",
            "F1 Score: 0.6066225165562914\n",
            "Accuracy: 0.43536121673003797\n",
            "\n",
            "-----------------------\n",
            "Powerset Classifier Chain\n",
            "Tiempo entrenamiento: 0.2200599193572998\n",
            "Tiempo prediccion: 0.02763199806213379\n",
            "F1 Score: 0.6307562952162277\n",
            "Accuracy: 0.4608109637090907\n",
            "\n",
            "-----------------------\n",
            "Homer Class Chain\n",
            "Tiempo entrenamiento: 0.18706755638122557\n",
            "Tiempo prediccion: 0.02287611961364746\n",
            "F1 Score: 0.6359444118222745\n",
            "Accuracy: 0.46621591291661985\n"
          ]
        }
      ],
      "source": [
        "evaluar_dataset(X_train,Y_train,X_test,Y_test,150,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvhC_rsWabGU"
      },
      "source": [
        "#**Birds**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY89YUeqaeTr",
        "outputId": "45140af8-f1e9-45d5-d29f-930c0e407895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "birds:train - does not exists downloading\n",
            "Downloaded birds-train\n",
            "birds:test - does not exists downloading\n",
            "Downloaded birds-test\n",
            "(322, 260)\n",
            "(322, 19)\n"
          ]
        }
      ],
      "source": [
        "# Scene Dataset\n",
        "X_train, Y_train, feature_names, label_names = load_dataset('birds', 'train')\n",
        "X_test, Y_test, feature_names, label_names = load_dataset('birds', 'test')\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "#print(X_train.toarray())\n",
        "#print(Y_train.toarray())\n",
        "\n",
        "X_train = X_train.toarray()\n",
        "Y_train = Y_train.toarray()\n",
        "X_test = X_test.toarray()\n",
        "Y_test = Y_test.toarray()\n",
        "\n",
        "X_train = StandardScaler().fit_transform(X_train)\n",
        "X_test = StandardScaler().fit_transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AspHxdAqogmd",
        "outputId": "6d30530c-9cf9-4dcf-a4e1-59b8cfbc94e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[18, 12, 7, 2, 9], [0, 5, 4, 11, 14], [16, 3, 13, 17, 15], [6, 8, 1, 10]]\n",
            "Fitting  15  classifiers...\n",
            "Done fitting\n",
            "[[6, 9, 8, 10], [12, 7, 18, 2, 1], [3, 16, 13, 17, 5], [15, 0, 4, 11, 14]]\n",
            "Fitting  15  classifiers...\n",
            "Done fitting\n",
            "[[16, 3, 13, 17, 15], [14, 2, 7, 8, 6], [18, 9, 1, 10], [0, 5, 11, 4, 12]]\n",
            "Fitting  15  classifiers...\n",
            "Done fitting\n",
            "[[16, 13, 17, 0, 11], [12, 6, 1, 10], [3, 5, 18, 2, 7], [15, 4, 14, 8, 9]]\n",
            "Fitting  15  classifiers...\n",
            "Done fitting\n",
            "[[16, 3, 13, 15, 4], [0, 5, 17, 18, 9], [11, 12, 14, 6, 1], [2, 7, 8, 10]]\n",
            "Fitting  15  classifiers...\n",
            "Done fitting\n",
            "-----------------------\n",
            "Classifier Chain\n",
            "Tiempo entrenamiento: 0.5789075374603272\n",
            "Tiempo prediccion: 0.041504859924316406\n",
            "F1 Score: 0.4350877192982455\n",
            "Accuracy: 0.27802690582959644\n",
            "\n",
            "-----------------------\n",
            "Powerset Classifier Chain\n",
            "Tiempo entrenamiento: 0.4663733959197998\n",
            "Tiempo prediccion: 0.0937422275543213\n",
            "F1 Score: 0.300337773495092\n",
            "Accuracy: 0.17695618647525282\n",
            "\n",
            "-----------------------\n",
            "Homer Class Chain\n",
            "Tiempo entrenamiento: 0.6754717350006103\n",
            "Tiempo prediccion: 0.12083368301391602\n",
            "F1 Score: 0.42429029083092795\n",
            "Accuracy: 0.2694186614091341\n"
          ]
        }
      ],
      "source": [
        "evaluar_dataset(X_train,Y_train,X_test,Y_test,150,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onsJDDb-oqNd"
      },
      "source": [
        "#**Mediamill**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrlNWRY-orbY",
        "outputId": "1919106d-326e-4292-d6e9-76d77bc3d8f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mediamill:train - does not exists downloading\n",
            "Downloaded mediamill-train\n",
            "mediamill:test - does not exists downloading\n",
            "Downloaded mediamill-test\n",
            "(30993, 120)\n",
            "(30993, 101)\n",
            "(12914, 120)\n"
          ]
        }
      ],
      "source": [
        "# Scene Dataset\n",
        "X_train, Y_train, feature_names, label_names = load_dataset('mediamill', 'train')\n",
        "X_test, Y_test, feature_names, label_names = load_dataset('mediamill', 'test')\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "#print(X_train.toarray())\n",
        "#print(Y_train.toarray())\n",
        "\n",
        "X_train = X_train.toarray()\n",
        "Y_train = Y_train.toarray()\n",
        "X_test = X_test.toarray()\n",
        "Y_test = Y_test.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEvxOdnVpCQ9",
        "outputId": "56bb63ee-bd59-4c2e-ca86-f36cd553ff67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[40, 23, 8, 1, 41], [80, 61, 38, 0, 79], [70, 59, 99, 85, 7], [11, 95, 93, 24, 43], [57, 56, 16, 96, 54], [75, 78, 97, 65, 31], [48, 87, 47, 74, 63], [37, 13, 76, 64, 60], [53, 49, 62, 71, 18], [55, 68, 44, 98, 22], [5, 50, 77, 46, 89], [27, 29, 9, 25, 14], [84, 51, 2], [88, 39, 4, 6, 86], [20, 15, 17, 28, 72], [81, 83, 94, 58, 34], [12, 32, 90, 10, 21], [35, 26, 69, 3, 92], [30, 36, 73, 42, 52], [19, 91, 45, 100, 82], [66, 67, 33]]\n",
            "Fitting  31  classifiers...\n",
            "Done fitting\n",
            "[[55, 60, 81, 68, 44], [32, 6, 90, 79, 4], [17, 28, 72, 74, 29], [27, 9, 25, 63, 73], [69, 38, 0, 100, 92], [41, 36, 42, 52, 62], [51, 24, 33, 67], [40, 14, 8, 1, 49], [5, 50, 77, 46, 99], [23, 71, 12, 30, 39], [48, 7, 85, 15, 47], [10, 21, 26, 82, 3], [16, 95, 75, 57, 11], [97, 96, 31], [98, 58, 94, 83, 34], [2, 54, 56, 43, 84], [70, 20, 89, 59, 87], [53, 88, 35, 80, 18], [37, 64, 76, 13, 22], [91, 86, 19, 61, 45], [93, 78, 65, 66]]\n",
            "Fitting  31  classifiers...\n",
            "Done fitting\n",
            "[[81, 98, 94, 83, 58], [71, 100, 55, 18, 0], [44, 2, 43, 54, 24], [41, 12, 39, 32, 88], [72, 74, 27, 63, 29], [84, 51, 31, 66], [25, 23, 1, 8, 30], [49, 90, 10, 91, 19], [16, 75, 95, 11, 93], [85, 15, 17, 28, 87], [35, 26, 79, 92, 82], [36, 4, 6, 86, 3], [33, 67], [48, 47, 9, 40, 14], [62, 61, 21, 45, 38], [69, 68, 22, 56, 34], [50, 89, 59, 99, 7], [42, 52, 53, 80, 73], [76, 64, 13, 60, 37], [5, 77, 46, 70, 20], [96, 57, 97, 78, 65]]\n",
            "Fitting  31  classifiers...\n",
            "Done fitting\n",
            "-----------------------\n",
            "Classifier Chain\n",
            "Tiempo entrenamiento: 197.14719581604004\n",
            "Tiempo prediccion: 1.6015527248382568\n",
            "F1 Score: 0.5057114418428287\n",
            "Accuracy: 0.338429575119344\n",
            "\n",
            "-----------------------\n",
            "Powerset Classifier Chain\n",
            "Tiempo entrenamiento: 317.87689717610675\n",
            "Tiempo prediccion: 17.259229977925617\n",
            "F1 Score: 0.5383265471629973\n",
            "Accuracy: 0.3682981822563973\n",
            "\n",
            "-----------------------\n",
            "Homer Class Chain\n",
            "Tiempo entrenamiento: 57.83932820955912\n",
            "Tiempo prediccion: 2.45372207959493\n",
            "F1 Score: 0.5497368789486691\n",
            "Accuracy: 0.3790613219647769\n"
          ]
        }
      ],
      "source": [
        "evaluar_dataset(X_train,Y_train,X_test,Y_test,450,10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pejqoOi-62e5"
      },
      "source": [
        "#**tmc2007_500**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vMTxAbH62I-",
        "outputId": "3e5101ee-dae4-49a4-a575-4ff96bac762e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tmc2007_500:train - exists, not redownloading\n",
            "tmc2007_500:test - exists, not redownloading\n",
            "(21519, 500)\n",
            "(21519, 22)\n",
            "(7077, 500)\n"
          ]
        }
      ],
      "source": [
        "# Scene Dataset\n",
        "X_train, Y_train, feature_names, label_names = load_dataset('tmc2007_500', 'train')\n",
        "X_test, Y_test, feature_names, label_names = load_dataset('tmc2007_500', 'test')\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "#print(X_train.toarray())\n",
        "#print(Y_train.toarray())\n",
        "\n",
        "X_train = X_train.toarray()\n",
        "Y_train = Y_train.toarray()\n",
        "X_test = X_test.toarray()\n",
        "Y_test = Y_test.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WsLiu7o7iGK",
        "outputId": "d0192ee2-de13-4f48-e9c8-b75a862a5bf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[8, 16, 21, 19, 15], [14, 20, 10, 2, 3], [9, 17, 13, 0, 7], [6, 12, 4, 5, 18], [1, 11]]\n",
            "Fitting  7  classifiers...\n",
            "Done fitting\n",
            "[[9, 15, 17, 6, 7], [16, 8, 3, 21, 13], [0, 12, 4, 11, 18], [14, 20, 10, 2, 19], [1, 5]]\n",
            "Fitting  7  classifiers...\n",
            "Done fitting\n",
            "[[3, 17, 13, 0, 6], [8, 16, 21, 15, 9], [7, 12, 4, 11, 5], [14, 20, 10, 2, 19], [1, 18]]\n",
            "Fitting  7  classifiers...\n",
            "Done fitting\n",
            "-----------------------\n",
            "Classifier Chain\n",
            "Tiempo entrenamiento: 72.80069414774577\n",
            "Tiempo prediccion: 0.5329740047454834\n",
            "F1 Score: 0.7088711329645564\n",
            "Accuracy: 0.5490320533164075\n",
            "\n",
            "-----------------------\n",
            "Powerset Classifier Chain\n",
            "Tiempo entrenamiento: 16.432985464731853\n",
            "Tiempo prediccion: 2.5953524907430015\n",
            "F1 Score: 0.9926594916194421\n",
            "Accuracy: 0.985426445563078\n",
            "\n",
            "-----------------------\n",
            "Homer Class Chain\n",
            "Tiempo entrenamiento: 53.23684024810791\n",
            "Tiempo prediccion: 1.0303197701772053\n",
            "F1 Score: 0.71344245663539\n",
            "Accuracy: 0.5545380579921628\n"
          ]
        }
      ],
      "source": [
        "evaluar_dataset(X_train,Y_train,X_test,Y_test,300,6)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Yeast**"
      ],
      "metadata": {
        "id": "hOmKIEK_iG-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scene Dataset\n",
        "X_train, Y_train, feature_names, label_names = load_dataset('yeast', 'train')\n",
        "X_test, Y_test, feature_names, label_names = load_dataset('yeast', 'test')\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "#print(X_train.toarray())\n",
        "#print(Y_train.toarray())\n",
        "\n",
        "X_train = X_train.toarray()\n",
        "Y_train = Y_train.toarray()\n",
        "X_test = X_test.toarray()\n",
        "Y_test = Y_test.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvJF6Q1FiJTD",
        "outputId": "63e39161-3494-492a-a306-cf58faf08847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yeast:train - does not exists downloading\n",
            "Downloaded yeast-train\n",
            "yeast:test - does not exists downloading\n",
            "Downloaded yeast-test\n",
            "(1500, 103)\n",
            "(1500, 14)\n",
            "(917, 103)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluar_dataset(X_train,Y_train,X_test,Y_test,150,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9v2zTchiJ5J",
        "outputId": "eebaf48c-0b21-4190-b631-928682f9623f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[12, 11, 4, 3], [9, 13, 8, 10, 2], [7, 6, 0, 1, 5]]\n",
            "Fitting  3  classifiers...\n",
            "Done fitting\n",
            "[[12, 11, 4, 5], [13, 9, 8, 10, 0], [7, 6, 2, 3, 1]]\n",
            "Fitting  3  classifiers...\n",
            "Done fitting\n",
            "[[6, 5, 7, 13, 8], [11, 12, 0, 4], [9, 2, 10, 3, 1]]\n",
            "Fitting  3  classifiers...\n",
            "Done fitting\n",
            "[[13, 9, 10, 8, 6], [0, 7, 5, 1], [12, 11, 3, 2, 4]]\n",
            "Fitting  3  classifiers...\n",
            "Done fitting\n",
            "[[8, 13, 9, 10, 7], [0, 6, 5, 1], [12, 11, 3, 2, 4]]\n",
            "Fitting  3  classifiers...\n",
            "Done fitting\n",
            "-----------------------\n",
            "Classifier Chain\n",
            "Tiempo entrenamiento: 0.3684194564819336\n",
            "Tiempo prediccion: 0.026512861251831055\n",
            "F1 Score: 0.6097318768619663\n",
            "Accuracy: 0.4385714285714285\n",
            "\n",
            "-----------------------\n",
            "Powerset Classifier Chain\n",
            "Tiempo entrenamiento: 1.467143440246582\n",
            "Tiempo prediccion: 0.21868138313293456\n",
            "F1 Score: 0.6267452581403309\n",
            "Accuracy: 0.45640470853379467\n",
            "\n",
            "-----------------------\n",
            "Homer Class Chain\n",
            "Tiempo entrenamiento: 0.466257905960083\n",
            "Tiempo prediccion: 0.08161706924438476\n",
            "F1 Score: 0.6481305123603345\n",
            "Accuracy: 0.4794385279197687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Enron**"
      ],
      "metadata": {
        "id": "_YxhP2KrxyOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scene Dataset\n",
        "X_train, Y_train, feature_names, label_names = load_dataset('enron', 'train')\n",
        "X_test, Y_test, feature_names, label_names = load_dataset('enron', 'test')\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "\n",
        "X_train = X_train.toarray()\n",
        "Y_train = Y_train.toarray()\n",
        "X_test = X_test.toarray()\n",
        "Y_test = Y_test.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNpnf7Umx0hS",
        "outputId": "24fa2e2d-372a-4ff6-e351-e7a2e48f3c20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enron:train - exists, not redownloading\n",
            "enron:test - exists, not redownloading\n",
            "(1123, 1001)\n",
            "(1123, 53)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Supongamos que 'y' es tu conjunto de etiquetas multietiqueta\n",
        "num_labels = Y_train.shape[1]  # Número de etiquetas\n",
        "etiq= []\n",
        "for label_idx in range(num_labels):\n",
        "    unique_classes = np.unique(Y_train[:, label_idx])  # Clases únicas en esta etiqueta\n",
        "    num_unique_classes = len(unique_classes)  # Número de clases únicas\n",
        "\n",
        "    if num_unique_classes != 2:\n",
        "        etiq+=[label_idx]\n",
        "print(etiq)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKKQpyqF102Y",
        "outputId": "c11788c2-d68c-4074-d046-66ce0787e09b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[45]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_idx_to_remove = 45  # Reemplaza con la posición de la etiqueta que deseas eliminar (0 para la primera etiqueta, 1 para la segunda, y así sucesivamente)\n",
        "\n",
        "# Crea un nuevo conjunto de etiquetas sin la etiqueta en la posición 'label_idx_to_remove'\n",
        "Y_train = np.delete(Y_train, label_idx_to_remove, axis=1)\n",
        "Y_test = np.delete(Y_test, label_idx_to_remove, axis=1)"
      ],
      "metadata": {
        "id": "jXcVCeK54ILK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluar_dataset(X_train,Y_train,X_test,Y_test,150,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub_UW1bcx2Jl",
        "outputId": "0644b841-45a6-4bc5-cb81-efa496a462fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 37, 18, 7, 42], [36, 19, 47, 28, 17], [3, 34, 49, 33, 9], [25, 6, 14], [13, 39, 45, 11], [4, 29, 12, 48, 21], [46, 51, 30, 2, 16], [41, 31, 22, 43, 24], [26, 8, 38, 10, 40], [1, 5, 20, 23, 44], [32, 27, 50, 15, 35]]\n",
            "Fitting  39  classifiers...\n",
            "Done fitting\n",
            "[[49, 1, 43, 5, 22], [29, 44, 48, 39, 20], [36, 19, 41, 37, 18], [30, 50, 15, 16, 26], [8, 3, 28, 33, 34], [25, 6, 11], [35, 38, 10, 47, 17], [45, 23, 21, 14], [46, 51, 2, 32, 27], [40, 9, 7, 0, 42], [31, 4, 12, 24, 13]]\n",
            "Fitting  39  classifiers...\n",
            "Done fitting\n",
            "[[28, 19, 49, 33, 18], [6, 11], [46, 51, 30, 2, 32], [27, 50, 15, 16, 8], [35, 26, 10, 36, 40], [41, 7, 31, 1, 43], [39, 45, 21, 25, 14], [9, 0, 37, 42, 22], [5, 24, 20, 23, 4], [38, 47, 17, 3, 34], [44, 12, 29, 13, 48]]\n",
            "Fitting  39  classifiers...\n",
            "Done fitting\n",
            "-----------------------\n",
            "Classifier Chain\n",
            "Tiempo entrenamiento: 8.01002812385559\n",
            "Tiempo prediccion: 0.2501362164815267\n",
            "F1 Score: 0.5448071216617211\n",
            "Accuracy: 0.37438825448613383\n",
            "\n",
            "-----------------------\n",
            "Powerset Classifier Chain\n",
            "Tiempo entrenamiento: 2.234882195790609\n",
            "Tiempo prediccion: 0.44085971514383954\n",
            "F1 Score: 0.5454817782444391\n",
            "Accuracy: 0.37503407770939373\n",
            "\n",
            "-----------------------\n",
            "Homer Class Chain\n",
            "Tiempo entrenamiento: 4.65871795018514\n",
            "Tiempo prediccion: 0.46582603454589844\n",
            "F1 Score: 0.5582291385370309\n",
            "Accuracy: 0.38720356317581883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Bibtex**"
      ],
      "metadata": {
        "id": "4vu9Alol5dN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scene Dataset\n",
        "X_train, Y_train, feature_names, label_names = load_dataset('bibtex', 'train')\n",
        "X_test, Y_test, feature_names, label_names = load_dataset('bibtex', 'test')\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "\n",
        "X_train = X_train.toarray()\n",
        "Y_train = Y_train.toarray()\n",
        "X_test = X_test.toarray()\n",
        "Y_test = Y_test.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5421483a-fac8-4f67-9540-5ded5cb0d7cc",
        "id": "h2w2Jbnx5dOW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bibtex:train - exists, not redownloading\n",
            "bibtex:test - exists, not redownloading\n",
            "(4880, 1836)\n",
            "(4880, 159)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluar_dataset(X_train,Y_train,X_test,Y_test,150,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef7e5ab5-cccd-4e74-bdc9-52c962a9c3d3",
        "id": "XDntoK7V5dOX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[50, 46, 92, 116, 99], [123, 137, 103, 20, 2], [45, 69, 147, 158, 31], [61, 9, 124, 6, 117], [33, 34, 4, 125, 87], [37, 40, 82, 115, 74], [80, 35, 16, 22, 70], [155, 48, 11, 15, 66], [107, 77, 119, 151, 42], [89, 71, 7, 19, 5], [108, 47, 150, 30, 142], [120, 51, 67, 126, 110], [144, 138, 96, 97, 141], [100, 85, 143, 149, 111], [84, 27, 113, 36, 83], [10, 131, 14, 134], [145, 114, 153, 90, 65], [133, 58, 98, 8, 78], [53, 29, 55, 121, 101], [130, 128, 59, 109, 17], [112, 0, 26, 127, 102], [68, 106, 43, 76, 154], [1, 18, 93, 146, 13], [12, 39, 132, 94, 60], [44, 41, 63, 129, 88], [57, 25, 140, 79, 148], [105, 64, 32, 73, 24], [56, 95, 72, 28, 136], [49, 23, 81, 3, 135], [122, 156, 104, 75, 52], [62, 38, 118, 91, 86], [139, 54, 157, 21, 152]]\n",
            "Fitting  31  classifiers...\n",
            "Done fitting\n",
            "[[76, 3, 61, 23, 119], [64, 65, 5, 114, 142], [157, 21, 139, 16, 1], [130, 59, 128, 0, 34], [143, 41, 141, 6, 117], [28, 136, 40, 58, 20], [68, 106, 43, 73, 121], [57, 140, 154, 110, 84], [22, 111, 9, 146, 96], [11, 85, 149, 18, 81], [126, 145, 107, 135, 27], [39, 17, 103, 51, 54], [127, 92, 45, 78, 132], [148, 108, 91, 47, 32], [90, 77, 42, 49, 155], [150, 26, 86, 153, 113], [30, 71, 7, 151, 15], [8, 12, 98, 112, 123], [37, 82, 115, 133, 74], [56, 95, 72, 158, 31], [25, 60, 120, 24, 67], [62, 38, 87, 118, 2], [50, 46, 69, 116, 99], [89, 48, 19, 35, 152], [144, 138, 97, 129, 83], [4, 125, 33, 102, 147], [100, 70, 93, 13, 124], [122, 156, 66, 104, 36], [44, 63, 88, 10, 52], [53, 29, 55, 101, 80], [75, 131, 14, 134], [109, 94, 79, 105, 137]]\n",
            "Fitting  31  classifiers...\n",
            "Done fitting\n",
            "[[92, 126, 145, 91, 24], [28, 136, 37, 82, 20], [60, 25, 105, 123, 73], [40, 102, 116, 31, 46], [62, 38, 118, 114, 90], [11, 61, 22, 85, 143], [34, 87, 33, 130, 67], [77, 27, 138, 113, 41], [68, 94, 79, 106, 120], [43, 30, 157, 107, 42], [44, 63, 36, 10, 75], [52, 131, 14, 134], [57, 12, 39, 109, 84], [100, 13, 66, 141, 117], [154, 64, 51, 110, 2], [53, 29, 55, 103, 101], [152, 18, 9, 144, 146], [16, 149, 81, 111, 83], [147, 50, 69, 99, 125], [133, 58, 115, 8, 98], [1, 122, 70, 156, 6], [71, 151, 5, 49, 80], [74, 4, 45, 59, 78], [65, 23, 32, 142, 86], [121, 135, 15, 119, 3], [54, 139, 21, 93, 124], [56, 95, 72, 158, 127], [112, 0, 26, 17, 132], [128, 140, 148, 137, 76], [47, 7, 108, 150, 35], [96, 97, 129, 104, 88], [89, 48, 19, 153, 155]]\n",
            "Fitting  31  classifiers...\n",
            "Done fitting\n",
            "-----------------------\n",
            "Classifier Chain\n",
            "Tiempo entrenamiento: 215.5844484170278\n",
            "Tiempo prediccion: 15.33990748723348\n",
            "F1 Score: 0.39985854061063303\n",
            "Accuracy: 0.24988949462207163\n",
            "\n",
            "-----------------------\n",
            "Powerset Classifier Chain\n",
            "Tiempo entrenamiento: 42.121177673339844\n",
            "Tiempo prediccion: 6.635937452316284\n",
            "F1 Score: 0.2762466257982781\n",
            "Accuracy: 0.1602615832778711\n",
            "\n",
            "-----------------------\n",
            "Homer Class Chain\n",
            "Tiempo entrenamiento: 33.27777600288391\n",
            "Tiempo prediccion: 2.1776952743530273\n",
            "F1 Score: 0.42855767734952016\n",
            "Accuracy: 0.27271641672904035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Delicious**"
      ],
      "metadata": {
        "id": "aika-FDWAWAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scene Dataset\n",
        "X_train, Y_train, feature_names, label_names = load_dataset('delicious', 'train')\n",
        "X_test, Y_test, feature_names, label_names = load_dataset('delicious', 'test')\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "\n",
        "X_train = X_train.toarray()\n",
        "Y_train = Y_train.toarray()\n",
        "X_test = X_test.toarray()\n",
        "Y_test = Y_test.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f20d2ece-4efc-452b-bb93-83f7f8e78c98",
        "id": "HgVwm-8AAWA6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "delicious:train - exists, not redownloading\n",
            "delicious:test - exists, not redownloading\n",
            "(12920, 500)\n",
            "(12920, 983)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluar_dataset(X_train,Y_train,X_test,Y_test,250,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "1160a2b6-760a-4af1-c9ad-bc6a5e5bc6f1",
        "id": "np3yvzFNAWA9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9, 28, 37, 36, 30], [14, 15, 25, 13, 29], [19, 20, 0, 26, 35], [2, 21, 23, 16, 10], [27, 4, 8], [3, 5, 6, 11, 12], [31, 1, 33, 18, 32], [7, 17, 22, 24, 34]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-105-ab34eb68ac97>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluar_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-98-15029caae3ef>\u001b[0m in \u001b[0;36mevaluar_dataset\u001b[0;34m(X_train, Y_train, X_test, Y_test, max_itera, leaf_cond)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Declaro un classifier chain basado en powersets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mpw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassChain_Powerset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mfin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mpw_train_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfin\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mini\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-76-974be639ee43>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, X_trainF, Y_trainF)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_trainF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_trainF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maplicar_kmeans_etiquetas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_trainF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentrenar_modelo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trainF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_trainF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mentrenar_modelo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_trainF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_trainF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# tengo que ajustar los parametros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-76-974be639ee43>\u001b[0m in \u001b[0;36mentrenar_modelo\u001b[0;34m(self, X_trainF, Y_trainF)\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelPowerset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0;31m#print(Y_train_list[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_aux\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mY_train_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlista_clasificadores\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skmultilearn/problem_transform/lp.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    138\u001b[0m             X, sparse_format='csr', enforce_sparse=True)\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         self.classifier.fit(self._ensure_input_format(X),\n\u001b[0m\u001b[1;32m    141\u001b[0m                             self.transform(y))\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    474\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \"\"\"\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    377\u001b[0m             )\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Medical**"
      ],
      "metadata": {
        "id": "_uvavY6HhZs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scene Dataset\n",
        "X_train, Y_train, feature_names, label_names = load_dataset('medical', 'train')\n",
        "X_test, Y_test, feature_names, label_names = load_dataset('medical', 'test')\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "\n",
        "X_train = X_train.toarray()\n",
        "Y_train = Y_train.toarray()\n",
        "X_test = X_test.toarray()\n",
        "Y_test = Y_test.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cee38ba5-ac85-4c09-e55c-6c5001ad4487",
        "id": "6sh-0RoqhZs5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "medical:train - does not exists downloading\n",
            "Downloaded medical-train\n",
            "medical:test - does not exists downloading\n",
            "Downloaded medical-test\n",
            "(333, 1449)\n",
            "(333, 45)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Supongamos que 'y' es tu conjunto de etiquetas multietiqueta\n",
        "num_labels = Y_train.shape[1]  # Número de etiquetas\n",
        "etiq= []\n",
        "for label_idx in range(num_labels):\n",
        "    unique_classes = np.unique(Y_train[:, label_idx])  # Clases únicas en esta etiqueta\n",
        "    num_unique_classes = len(unique_classes)  # Número de clases únicas\n",
        "\n",
        "    if num_unique_classes != 2:\n",
        "        etiq+=[label_idx]\n",
        "print(etiq)"
      ],
      "metadata": {
        "id": "K69g56NKh6pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluar_dataset(X_train,Y_train,X_test,Y_test,250,8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d7355b1-ac62-4d9e-fa30-4b49e7bfc4f1",
        "id": "cTSCYn0_hZs6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[37, 20, 26, 0, 35], [27, 4, 8], [1, 9, 29, 33, 18], [14, 25, 10, 13, 31], [3, 5, 6, 7, 11], [12, 17, 24, 34, 2], [22, 21, 23, 15, 16], [32, 28, 19, 36, 30]]\n",
            "Fitting  15  classifiers...\n",
            "Done fitting\n",
            "[[32, 28, 19, 36, 37], [30, 20, 35, 0, 26], [1, 9, 29, 33, 18], [27, 4, 8], [14, 25, 10, 13, 31], [3, 5, 11, 12, 17], [7, 22, 24, 34, 2], [6, 21, 23, 15, 16]]\n",
            "Fitting  15  classifiers...\n",
            "Done fitting\n",
            "[[3, 5, 7, 11, 22], [21, 25, 31, 1, 9], [10, 29, 33, 18, 30], [36, 20, 0, 26, 35], [12, 17, 24, 34, 2], [6, 23, 14, 15, 16], [27, 4, 8], [13, 32, 28, 19, 37]]\n",
            "Fitting  15  classifiers...\n",
            "Done fitting\n",
            "-----------------------\n",
            "Classifier Chain\n",
            "Tiempo entrenamiento: 1.4407409032185872\n",
            "Tiempo prediccion: 0.21867974599202475\n",
            "F1 Score: 0.6863636363636362\n",
            "Accuracy: 0.5224913494809689\n",
            "\n",
            "-----------------------\n",
            "Powerset Classifier Chain\n",
            "Tiempo entrenamiento: 0.9213058153788248\n",
            "Tiempo prediccion: 0.35993361473083496\n",
            "F1 Score: 0.6669534318806715\n",
            "Accuracy: 0.5004623716529512\n",
            "\n",
            "-----------------------\n",
            "Homer Class Chain\n",
            "Tiempo entrenamiento: 0.957151730855306\n",
            "Tiempo prediccion: 0.2292481263478597\n",
            "F1 Score: 0.7507378181243175\n",
            "Accuracy: 0.6009512263738265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Wilcoxon\n"
      ],
      "metadata": {
        "id": "XHeSegsG5xoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "better_is_higher = True\n",
        "def Wilcoxon(x, y=None, zero_method=\"wilcox\", correction=False):\n",
        "    \"\"\"\n",
        "    Calculate the Wilcoxon signed-rank test.\n",
        "\n",
        "    The Wilcoxon signed-rank test tests the null hypothesis that two\n",
        "    related paired samples come from the same distribution. In particular,\n",
        "    it tests whether the distribution of the differences x - y is symmetric\n",
        "    about zero. It is a non-parametric version of the paired T-test.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : array_like\n",
        "        The first set of measurements.\n",
        "    y : array_like, optional\n",
        "        The second set of measurements.  If `y` is not given, then the `x`\n",
        "        array is considered to be the differences between the two sets of\n",
        "        measurements.\n",
        "    zero_method : string, {\"pratt\", \"wilcox\", \"zsplit\"}, optional\n",
        "        \"pratt\":\n",
        "            Pratt treatment: includes zero-differences in the ranking process\n",
        "            (more conservative)\n",
        "        \"wilcox\":\n",
        "            Wilcox treatment: discards all zero-differences\n",
        "        \"zsplit\":\n",
        "            Zero rank split: just like Pratt, but spliting the zero rank\n",
        "            between positive and negative ones\n",
        "    correction : bool, optional\n",
        "        If True, apply continuity correction by adjusting the Wilcoxon rank\n",
        "        statistic by 0.5 towards the mean value when computing the\n",
        "        z-statistic.  Default is False.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    T : float\n",
        "        The sum of the ranks of the differences above or below zero, whichever\n",
        "        is smaller.\n",
        "    p-value : float\n",
        "        The two-sided p-value for the test.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    Because the normal approximation is used for the calculations, the\n",
        "    samples used should be large.  A typical rule is to require that\n",
        "    n > 20.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] http://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if not zero_method in [\"wilcox\", \"pratt\", \"zsplit\"]:\n",
        "        raise ValueError(\"Zero method should be either 'wilcox' or 'pratt' or 'zsplit'\")\n",
        "\n",
        "    if y is None:\n",
        "        d = x\n",
        "    else:\n",
        "        x, y = map(asarray, (x, y))\n",
        "        if len(x) != len(y):\n",
        "            raise ValueError('Unequal N in wilcoxon.  Aborting.')\n",
        "        if better_is_higher:\n",
        "            d = x-y\n",
        "        else:\n",
        "            d = y-x\n",
        "\n",
        "    if zero_method == \"wilcox\":\n",
        "        d = compress(not_equal(d, 0), d, axis=-1)  # Keep all non-zero differences\n",
        "\n",
        "    count = len(d)\n",
        "    #if (count < 10):\n",
        "     #   print(\"Warning: sample size too small for normal approximation.\")\n",
        "      #  return\n",
        "    r = stats.rankdata(abs(d))\n",
        "    r_plus = sum((d > 0) * r, axis=0)\n",
        "    r_minus = sum((d < 0) * r, axis=0)\n",
        "\n",
        "    if zero_method == \"zsplit\":\n",
        "        r_zero = sum((d == 0) * r, axis=0)\n",
        "        r_plus += r_zero/2.\n",
        "        r_minus += r_zero/2.\n",
        "\n",
        "    # T = np.min(r_plus, r_minus)   It does not work, I don't know why!\n",
        "    T = r_plus if r_plus < r_minus else r_minus\n",
        "    mn = count*(count + 1.) * 0.25\n",
        "    se = count*(count + 1.) * (2. * count + 1.)\n",
        "\n",
        "    if zero_method == \"pratt\":\n",
        "        r = r[d != 0]\n",
        "\n",
        "    replist, repnum = find_repeats(r)\n",
        "    if repnum.size != 0:\n",
        "        # Correction for repeated elements.\n",
        "        se -= 0.5 * (repnum * (repnum * repnum - 1)).sum()\n",
        "\n",
        "    se = sqrt(se/24)\n",
        "    correction = 0.5 * int(bool(correction)) * sign(T - mn)\n",
        "    z = (T - mn - correction)/se\n",
        "    prob = 2. * distributions.norm.sf(abs(z))\n",
        "    return r_minus, r_plus, prob\n",
        "\n",
        "# Get average value of 1 or more columns\n",
        "def AverageColums(c, clist, w):\n",
        "    v = [(float(c[i]) if i < len(c) else 0.0) for i in clist]\n",
        "    if sum(w) <= 0.0:   # If all weights are 0 use geometric mean\n",
        "        mean  =  gmean(v)\n",
        "    else:\n",
        "        mean = sum([x*y for x,y in zip(v, w)])\n",
        "\n",
        "    return mean"
      ],
      "metadata": {
        "id": "ODdkiBjR52EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import math\n",
        "from numpy import *\n",
        "from scipy.stats import wilcoxon, ttest_ind, mannwhitneyu, find_repeats, distributions, norm, rankdata, f, gmean\n",
        "from scipy import stats\n",
        "from pathlib import Path\n",
        "import argparse\n",
        "from scipy.stats import studentized_range"
      ],
      "metadata": {
        "id": "Zr1DrTcg6mPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Wilcoxon([0.1,0.2],[0.2,0.3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nMqb_x_56ws",
        "outputId": "e261782b-b244-4e0b-d263-c8818b93e8f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3.0, 0.0, 0.17971249487899976)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pFQyNoBDZiyp",
        "lS8OR7mptSSh",
        "iN0QG08YPOJ8",
        "r-2llWv6Q7BM",
        "7JuK3gz-RAWu",
        "fvhC_rsWabGU",
        "onsJDDb-oqNd",
        "pejqoOi-62e5",
        "hOmKIEK_iG-X",
        "_YxhP2KrxyOM",
        "4vu9Alol5dN5",
        "XHeSegsG5xoh"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyPf3jird0vp2F4VvpEFQnuS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}